{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "**Due on Thursday, Febrary 21, 2.00pm**\n",
    "\n",
    "Your submission will be a pdf of your Jupyter notebook which would have already ran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.** You are given the following list of English vowels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word: computation\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "vowels = [\"a\", \"o\", \"i\", \"u\", \"e\"]\n",
    "word = input(\"Enter a word: \")\n",
    "consonant_count = 0\n",
    "for letter in word:\n",
    "\tif letter not in vowels:\n",
    "\t\tconsonant_count += 1\n",
    "print(consonant_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the idea of a counter, implement a program that asks the user for a word, and then prints the number of consonants in that word. (For simplicity, we assume that \"y\" always behaves as a consonant, even though it is not true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2.**\n",
    "Implement a program that asks the user for the value of $n$ and for a word, and extracts $n$-grams from that word for any $n$ provided by the user.\n",
    "\n",
    "    word:   banana\n",
    "    n:      2\n",
    "    ngrams: ba, an, na, an, na\n",
    "    \n",
    "    word:   linguist\n",
    "    n:      3\n",
    "    ngrams: lin, ing, ngu, gui, uis, ist\n",
    "    \n",
    "    \n",
    "*Hint 1* If you didn't do it for practice before, start by implementing a code that extracts all bigrams. Then think about how you can generalize it to arbitary $n$.\n",
    "\n",
    "*Hint 2* Be careful with the *edges* (i.e., the last $n$ gram in each word). And what happens if the word is shorter then the $n$-gram? (i.e, the word is \"hi\" and n=3? You still need to list \"hi\"!)\n",
    "\n",
    "**Important** There are multiple default libraries to extract $n$-grams, already available in Python. But for this homework you **must** use the concepts we have studies so far. Any solution involving an external library will be counted as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word: linguistics\n",
      "Enter a number for n: 3\n",
      "['lin', 'ing', 'ngu', 'gui', 'uis', 'ist', 'sti', 'tic', 'ics']\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter a word: \")\n",
    "bigrams = []\n",
    "n = int(input(\"Enter a number for n: \"))\n",
    "\n",
    "for i in range(len(word) - n + 1):\n",
    "\tbigram = word[i:i+n]\n",
    "\tbigrams.append(bigram)\n",
    "if len(word) < n:\n",
    "\tbigrams.append(word)\n",
    "\t\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.** You are given the following text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"It was dark, like the bottom of a well. There was a pattern of skulls and bones around \\\n",
    "the frame, for the sake of appearances; Death could not look himself in the skull in a mirror \\\n",
    "with cherubs and roses around it. The Death of Rats climbed the frame in a scrabble of claws and \\\n",
    "looked at Death expectantly from the top. Quoth fluttered over and pecked briefly at his own \\\n",
    "reflection, on the basis that anything was worth a try. Show me, said Death, show me my thoughts. \\\n",
    "A chessboard appeared, but it was triangular, and so big that only the nearest point could be seen. \\\n",
    "Right on this point was the world - turtle, elephants, the little orbiting sun and all. It was the \\\n",
    "Discworld, which existed only just this side of total improbability and, therefore, in border country. \\\n",
    "In border country the border gets crossed, and sometimes things creep into the universe that have \\\n",
    "rather more on their mind than a better life for their children and a wonderful future in the \\\n",
    "fruit picking and domestic service industries. On every other black or white triangle of the \\\n",
    "chessboard, all the way to infinity, was a small grey shape, rather like an empty hooded robe.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also given a string that contains all symbols of English alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part 1._ Write some code that generates the list `unique_words`, containing all and only the unique lowercase words from `text`.\n",
    "\n",
    "You should see the following output (the order can differ!):\n",
    "    \n",
    "    ['a', 'infinity', 'reflection', 'with', 'like', 'big', 'briefly', 'into', 'children', 'which', 'fruit', 'picking', 'there', 'try', 'little', 'around', 'appearances', 'appeared', 'all', 'crossed', 'basis', 'improbability', 'their', 'discworld', 'black', 'to', 'death', 'future', 'only', 'my', 'robe', 'things', 'for', 'it', 'existed', 'said', 'sake', 'sometimes', 'right', 'way', 'that', 'country', 'chessboard', 'quoth', 'well', 'domestic', 'skull', 'wonderful', 'hooded', 'or', 'empty', 'bottom', 'mirror', 'himself', 'rather', 'over', 'every', 'triangle', 'roses', 'border', 'orbiting', 'was', 'from', 'show', 'be', 'pecked', 'bones', 'just', 'universe', 'me', 'triangular', 'gets', 'worth', 'have', 'climbed', 'service', 'fluttered', 'top', 'but', 'grey', 'claws', 'at', 'rats', 'creep', 'own', 'pattern', 'point', 'white', 'than', 'dark', 'therefore', 'frame', 'this', 'not', 'the', 'could', 'mind', 'turtle', 'scrabble', 'better', 'industries', 'looked', 'an', 'cherubs', 'life', 'anything', 'more', 'small', 'and', 'of', 'his', 'on', 'skulls', 'elephants', 'in', 'thoughts', 'seen', 'nearest', 'expectantly', 'other', 'side', 'shape', 'total', 'so', 'world', 'look', 'sun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dark', 'bottom', 'well', 'there', 'pattern', 'skulls', 'bones', 'sake', 'appearances', 'not', 'look', 'himself', 'skull', 'mirror', 'with', 'cherubs', 'roses', 'rats', 'climbed', 'scrabble', 'claws', 'looked', 'expectantly', 'from', 'top', 'quoth', 'fluttered', 'over', 'pecked', 'briefly', 'his', 'own', 'reflection', 'basis', 'anything', 'worth', 'try', 'said', 'my', 'thoughts', 'appeared', 'but', 'triangular', 'so', 'big', 'nearest', 'be', 'seen', 'right', 'world', 'turtle', 'elephants', 'little', 'orbiting', 'sun', 'discworld', 'which', 'existed', 'just', 'side', 'total', 'improbability', 'therefore', 'gets', 'crossed', 'sometimes', 'things', 'creep', 'into', 'universe', 'have', 'more', 'mind', 'than', 'better', 'life', 'children', 'wonderful', 'future', 'fruit', 'picking', 'domestic', 'service', 'industries', 'every', 'other', 'black', 'or', 'white', 'triangle', 'way', 'to', 'infinity', 'small', 'grey', 'shape', 'an', 'empty', 'hooded', 'robe']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "new_text = \"\"\n",
    "word_freq = {}\n",
    "unique_words = []\n",
    "unique_count = 0\n",
    "\n",
    "for char in text.lower():\n",
    "    if char in alphabet or char == \" \":\n",
    "        new_text += char\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "words = new_text.split(\" \")\n",
    "\n",
    "for word in words:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "         word_freq[word] = 1\n",
    "    \n",
    "for word in words:\n",
    "    if word_freq[word] == 1 and word != \"\":\n",
    "        unique_words.append(word)\n",
    "        unique_count += 1\n",
    "\n",
    "print(unique_words)\n",
    "print(unique_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part 2._ Write a program which generates the list `bigrams`, collect all attested bigrams in `unique_words`. Ignore words that are shorter than $2$ characters. **Make sure that the list `bigrams` does not contain duplicates**.\n",
    "\n",
    "*Hint. You can use the code to extract bigrams you wrote above. Then, you need to have that code iterate over each word in the `unique_words` list and add a check for duplicates!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['da', 'ar', 'rk', 'bo', 'ot', 'tt', 'to', 'om', 'we', 'el', 'll', 'th', 'he', 'er', 're', 'pa', 'at', 'te', 'rn', 'sk', 'ku', 'ul', 'ls', 'on', 'ne', 'es', 'sa', 'ak', 'ke', 'ap', 'pp', 'pe', 'ea', 'ra', 'an', 'nc', 'ce', 'no', 'lo', 'oo', 'ok', 'hi', 'im', 'ms', 'se', 'lf', 'mi', 'ir', 'rr', 'ro', 'or', 'wi', 'it', 'ch', 'ru', 'ub', 'bs', 'os', 'ts', 'cl', 'li', 'mb', 'be', 'ed', 'sc', 'cr', 'ab', 'bb', 'bl', 'le', 'la', 'aw', 'ws', 'ex', 'xp', 'ec', 'ct', 'ta', 'nt', 'tl', 'ly', 'fr', 'op', 'qu', 'uo', 'fl', 'lu', 'ut', 'ov', 've', 'ck', 'br', 'ri', 'ie', 'ef', 'is', 'ow', 'wn', 'ti', 'io', 'ba', 'as', 'si', 'ny', 'yt', 'in', 'ng', 'wo', 'rt', 'tr', 'ry', 'ai', 'id', 'ho', 'ou', 'ug', 'gh', 'ht', 'bu', 'ia', 'gu', 'bi', 'ig', 'st', 'ee', 'en', 'rl', 'ld', 'tu', 'ur', 'ep', 'ph', 'ha', 'rb', 'su', 'un', 'di', 'cw', 'wh', 'ic', 'xi', 'ju', 'us', 'de', 'al', 'mp', 'pr', 'ob', 'il', 'ty', 'fo', 'ge', 'et', 'ss', 'so', 'me', 'gs', 'ni', 'iv', 'rs', 'av', 'mo', 'nd', 'if', 'fe', 'dr', 'rf', 'fu', 'ui', 'pi', 'ki', 'do', 'rv', 'vi', 'du', 'ev', 'ac', 'gl', 'wa', 'ay', 'nf', 'fi', 'sm', 'ma', 'gr', 'ey', 'sh', 'em', 'pt', 'od'] 190\n"
     ]
    }
   ],
   "source": [
    "bigrams = []\n",
    "n = 2\n",
    "\n",
    "for word in unique_words:\n",
    "    if len(word) > 2:\n",
    "        for i in range(len(word) - n + 1):\n",
    "            bigram = word[i:i+n]\n",
    "            if bigram not in bigrams:\n",
    "                bigrams.append(bigram)\n",
    "\n",
    "print(bigrams, len(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part 3._ Based on the variable `alphabet`, generate all possible bigrams of English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh', 'bi', 'bj', 'bk', 'bl', 'bm', 'bn', 'bo', 'bp', 'bq', 'br', 'bs', 'bt', 'bu', 'bv', 'bw', 'bx', 'by', 'bz', 'ca', 'cb', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'ck', 'cl', 'cm', 'cn', 'co', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cw', 'cx', 'cy', 'cz', 'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'dh', 'di', 'dj', 'dk', 'dl', 'dm', 'dn', 'do', 'dp', 'dq', 'dr', 'ds', 'dt', 'du', 'dv', 'dw', 'dx', 'dy', 'dz', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'eh', 'ei', 'ej', 'ek', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ew', 'ex', 'ey', 'ez', 'fa', 'fb', 'fc', 'fd', 'fe', 'ff', 'fg', 'fh', 'fi', 'fj', 'fk', 'fl', 'fm', 'fn', 'fo', 'fp', 'fq', 'fr', 'fs', 'ft', 'fu', 'fv', 'fw', 'fx', 'fy', 'fz', 'ga', 'gb', 'gc', 'gd', 'ge', 'gf', 'gg', 'gh', 'gi', 'gj', 'gk', 'gl', 'gm', 'gn', 'go', 'gp', 'gq', 'gr', 'gs', 'gt', 'gu', 'gv', 'gw', 'gx', 'gy', 'gz', 'ha', 'hb', 'hc', 'hd', 'he', 'hf', 'hg', 'hh', 'hi', 'hj', 'hk', 'hl', 'hm', 'hn', 'ho', 'hp', 'hq', 'hr', 'hs', 'ht', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ih', 'ii', 'ij', 'ik', 'il', 'im', 'in', 'io', 'ip', 'iq', 'ir', 'is', 'it', 'iu', 'iv', 'iw', 'ix', 'iy', 'iz', 'ja', 'jb', 'jc', 'jd', 'je', 'jf', 'jg', 'jh', 'ji', 'jj', 'jk', 'jl', 'jm', 'jn', 'jo', 'jp', 'jq', 'jr', 'js', 'jt', 'ju', 'jv', 'jw', 'jx', 'jy', 'jz', 'ka', 'kb', 'kc', 'kd', 'ke', 'kf', 'kg', 'kh', 'ki', 'kj', 'kk', 'kl', 'km', 'kn', 'ko', 'kp', 'kq', 'kr', 'ks', 'kt', 'ku', 'kv', 'kw', 'kx', 'ky', 'kz', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'lg', 'lh', 'li', 'lj', 'lk', 'll', 'lm', 'ln', 'lo', 'lp', 'lq', 'lr', 'ls', 'lt', 'lu', 'lv', 'lw', 'lx', 'ly', 'lz', 'ma', 'mb', 'mc', 'md', 'me', 'mf', 'mg', 'mh', 'mi', 'mj', 'mk', 'ml', 'mm', 'mn', 'mo', 'mp', 'mq', 'mr', 'ms', 'mt', 'mu', 'mv', 'mw', 'mx', 'my', 'mz', 'na', 'nb', 'nc', 'nd', 'ne', 'nf', 'ng', 'nh', 'ni', 'nj', 'nk', 'nl', 'nm', 'nn', 'no', 'np', 'nq', 'nr', 'ns', 'nt', 'nu', 'nv', 'nw', 'nx', 'ny', 'nz', 'oa', 'ob', 'oc', 'od', 'oe', 'of', 'og', 'oh', 'oi', 'oj', 'ok', 'ol', 'om', 'on', 'oo', 'op', 'oq', 'or', 'os', 'ot', 'ou', 'ov', 'ow', 'ox', 'oy', 'oz', 'pa', 'pb', 'pc', 'pd', 'pe', 'pf', 'pg', 'ph', 'pi', 'pj', 'pk', 'pl', 'pm', 'pn', 'po', 'pp', 'pq', 'pr', 'ps', 'pt', 'pu', 'pv', 'pw', 'px', 'py', 'pz', 'qa', 'qb', 'qc', 'qd', 'qe', 'qf', 'qg', 'qh', 'qi', 'qj', 'qk', 'ql', 'qm', 'qn', 'qo', 'qp', 'qq', 'qr', 'qs', 'qt', 'qu', 'qv', 'qw', 'qx', 'qy', 'qz', 'ra', 'rb', 'rc', 'rd', 're', 'rf', 'rg', 'rh', 'ri', 'rj', 'rk', 'rl', 'rm', 'rn', 'ro', 'rp', 'rq', 'rr', 'rs', 'rt', 'ru', 'rv', 'rw', 'rx', 'ry', 'rz', 'sa', 'sb', 'sc', 'sd', 'se', 'sf', 'sg', 'sh', 'si', 'sj', 'sk', 'sl', 'sm', 'sn', 'so', 'sp', 'sq', 'sr', 'ss', 'st', 'su', 'sv', 'sw', 'sx', 'sy', 'sz', 'ta', 'tb', 'tc', 'td', 'te', 'tf', 'tg', 'th', 'ti', 'tj', 'tk', 'tl', 'tm', 'tn', 'to', 'tp', 'tq', 'tr', 'ts', 'tt', 'tu', 'tv', 'tw', 'tx', 'ty', 'tz', 'ua', 'ub', 'uc', 'ud', 'ue', 'uf', 'ug', 'uh', 'ui', 'uj', 'uk', 'ul', 'um', 'un', 'uo', 'up', 'uq', 'ur', 'us', 'ut', 'uu', 'uv', 'uw', 'ux', 'uy', 'uz', 'va', 'vb', 'vc', 'vd', 've', 'vf', 'vg', 'vh', 'vi', 'vj', 'vk', 'vl', 'vm', 'vn', 'vo', 'vp', 'vq', 'vr', 'vs', 'vt', 'vu', 'vv', 'vw', 'vx', 'vy', 'vz', 'wa', 'wb', 'wc', 'wd', 'we', 'wf', 'wg', 'wh', 'wi', 'wj', 'wk', 'wl', 'wm', 'wn', 'wo', 'wp', 'wq', 'wr', 'ws', 'wt', 'wu', 'wv', 'ww', 'wx', 'wy', 'wz', 'xa', 'xb', 'xc', 'xd', 'xe', 'xf', 'xg', 'xh', 'xi', 'xj', 'xk', 'xl', 'xm', 'xn', 'xo', 'xp', 'xq', 'xr', 'xs', 'xt', 'xu', 'xv', 'xw', 'xx', 'xy', 'xz', 'ya', 'yb', 'yc', 'yd', 'ye', 'yf', 'yg', 'yh', 'yi', 'yj', 'yk', 'yl', 'ym', 'yn', 'yo', 'yp', 'yq', 'yr', 'ys', 'yt', 'yu', 'yv', 'yw', 'yx', 'yy', 'yz', 'za', 'zb', 'zc', 'zd', 'ze', 'zf', 'zg', 'zh', 'zi', 'zj', 'zk', 'zl', 'zm', 'zn', 'zo', 'zp', 'zq', 'zr', 'zs', 'zt', 'zu', 'zv', 'zw', 'zx', 'zy', 'zz']\n"
     ]
    }
   ],
   "source": [
    "english_bigrams = []\n",
    "for letter1 in alphabet:\n",
    "    for letter2 in alphabet:\n",
    "        bigram = letter1 + letter2\n",
    "        if bigram not in english_bigrams:\n",
    "            english_bigrams.append(bigram)\n",
    "    if bigram not in english_bigrams:\n",
    "        english_bigrams.append(bigram)\n",
    "print(english_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part 4._ Collect all unattested bigrams of English in the list `unattested_bigrams`. \n",
    "\n",
    "*Hint* The unattested bigrams are those bigrams that are possible but not attested in the word sample (you collected all attested bigrams before)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be surprised that some bigrams from `unattested_bigrams` are actually present in other English words, the text that we are working with is very small! If you are curious, take a larger text, and run your code on it. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams of the sample text: ['it', 'wa', 'as', 'da', 'ar', 'rk', 'li', 'ik', 'ke', 'th', 'he', 'bo', 'ot', 'tt', 'to', 'om', 'of', 'we', 'el', 'll', 'er', 're', 'pa', 'at', 'te', 'rn', 'sk', 'ku', 'ul', 'ls', 'an', 'nd', 'on', 'ne', 'es', 'ro', 'ou', 'un', 'fr', 'ra', 'am', 'me', 'fo', 'or', 'sa', 'ak', 'ap', 'pp', 'pe', 'ea', 'nc', 'ce', 'de', 'co', 'ld', 'no', 'lo', 'oo', 'ok', 'hi', 'im', 'ms', 'se', 'lf', 'in', 'mi', 'ir', 'rr', 'wi', 'ch', 'ru', 'ub', 'bs', 'os', 'ts', 'cl', 'mb', 'be', 'ed', 'sc', 'cr', 'ab', 'bb', 'bl', 'le', 'la', 'aw', 'ws', 'ex', 'xp', 'ec', 'ct', 'ta', 'nt', 'tl', 'ly', 'op', 'qu', 'uo', 'fl', 'lu', 'ut', 'ov', 've', 'ck', 'br', 'ri', 'ie', 'ef', 'is', 'ow', 'wn', 'ti', 'io', 'ba', 'si', 'ha', 'ny', 'yt', 'ng', 'wo', 'rt', 'tr', 'ry', 'sh', 'ho', 'ai', 'id', 'my', 'ug', 'gh', 'ht', 'ss', 'sb', 'oa', 'rd', 'bu', 'ia', 'gu', 'so', 'bi', 'ig', 'nl', 'st', 'po', 'oi', 'ee', 'en', 'rl', 'tu', 'ur', 'ep', 'ph', 'rb', 'su', 'al', 'di', 'cw', 'wh', 'ic', 'xi', 'ju', 'us', 'mp', 'pr', 'ob', 'il', 'ty', 'ge', 'et', 'gs', 'ni', 'iv', 'rs', 'av', 'mo', 'ei', 'if', 'fe', 'dr', 'rf', 'fu', 'ui', 'pi', 'ki', 'do', 'rv', 'vi', 'du', 'ev', 'ac', 'gl', 'ay', 'nf', 'fi', 'sm', 'ma', 'gr', 'ey', 'em', 'pt', 'od'] 202\n",
      "\n",
      "Unattested bigrams: ['aa', 'ad', 'ae', 'af', 'ag', 'ah', 'aj', 'ao', 'aq', 'au', 'ax', 'az', 'bc', 'bd', 'bf', 'bg', 'bh', 'bj', 'bk', 'bm', 'bn', 'bp', 'bq', 'bt', 'bv', 'bw', 'bx', 'by', 'bz', 'ca', 'cb', 'cc', 'cd', 'cf', 'cg', 'ci', 'cj', 'cm', 'cn', 'cp', 'cq', 'cs', 'cu', 'cv', 'cx', 'cy', 'cz', 'db', 'dc', 'dd', 'df', 'dg', 'dh', 'dj', 'dk', 'dl', 'dm', 'dn', 'dp', 'dq', 'ds', 'dt', 'dv', 'dw', 'dx', 'dy', 'dz', 'eb', 'eg', 'eh', 'ej', 'ek', 'eo', 'eq', 'eu', 'ew', 'ez', 'fa', 'fb', 'fc', 'fd', 'ff', 'fg', 'fh', 'fj', 'fk', 'fm', 'fn', 'fp', 'fq', 'fs', 'ft', 'fv', 'fw', 'fx', 'fy', 'fz', 'ga', 'gb', 'gc', 'gd', 'gf', 'gg', 'gi', 'gj', 'gk', 'gm', 'gn', 'go', 'gp', 'gq', 'gt', 'gv', 'gw', 'gx', 'gy', 'gz', 'hb', 'hc', 'hd', 'hf', 'hg', 'hh', 'hj', 'hk', 'hl', 'hm', 'hn', 'hp', 'hq', 'hr', 'hs', 'hu', 'hv', 'hw', 'hx', 'hy', 'hz', 'ib', 'ih', 'ii', 'ij', 'ip', 'iq', 'iu', 'iw', 'ix', 'iy', 'iz', 'ja', 'jb', 'jc', 'jd', 'je', 'jf', 'jg', 'jh', 'ji', 'jj', 'jk', 'jl', 'jm', 'jn', 'jo', 'jp', 'jq', 'jr', 'js', 'jt', 'jv', 'jw', 'jx', 'jy', 'jz', 'ka', 'kb', 'kc', 'kd', 'kf', 'kg', 'kh', 'kj', 'kk', 'kl', 'km', 'kn', 'ko', 'kp', 'kq', 'kr', 'ks', 'kt', 'kv', 'kw', 'kx', 'ky', 'kz', 'lb', 'lc', 'lg', 'lh', 'lj', 'lk', 'lm', 'ln', 'lp', 'lq', 'lr', 'lt', 'lv', 'lw', 'lx', 'lz', 'mc', 'md', 'mf', 'mg', 'mh', 'mj', 'mk', 'ml', 'mm', 'mn', 'mq', 'mr', 'mt', 'mu', 'mv', 'mw', 'mx', 'mz', 'na', 'nb', 'nh', 'nj', 'nk', 'nm', 'nn', 'np', 'nq', 'nr', 'ns', 'nu', 'nv', 'nw', 'nx', 'nz', 'oc', 'oe', 'og', 'oh', 'oj', 'ol', 'oq', 'ox', 'oy', 'oz', 'pb', 'pc', 'pd', 'pf', 'pg', 'pj', 'pk', 'pl', 'pm', 'pn', 'pq', 'ps', 'pu', 'pv', 'pw', 'px', 'py', 'pz', 'qa', 'qb', 'qc', 'qd', 'qe', 'qf', 'qg', 'qh', 'qi', 'qj', 'qk', 'ql', 'qm', 'qn', 'qo', 'qp', 'qq', 'qr', 'qs', 'qt', 'qv', 'qw', 'qx', 'qy', 'qz', 'rc', 'rg', 'rh', 'rj', 'rm', 'rp', 'rq', 'rw', 'rx', 'rz', 'sd', 'sf', 'sg', 'sj', 'sl', 'sn', 'sp', 'sq', 'sr', 'sv', 'sw', 'sx', 'sy', 'sz', 'tb', 'tc', 'td', 'tf', 'tg', 'tj', 'tk', 'tm', 'tn', 'tp', 'tq', 'tv', 'tw', 'tx', 'tz', 'ua', 'uc', 'ud', 'ue', 'uf', 'uh', 'uj', 'uk', 'um', 'up', 'uq', 'uu', 'uv', 'uw', 'ux', 'uy', 'uz', 'va', 'vb', 'vc', 'vd', 'vf', 'vg', 'vh', 'vj', 'vk', 'vl', 'vm', 'vn', 'vo', 'vp', 'vq', 'vr', 'vs', 'vt', 'vu', 'vv', 'vw', 'vx', 'vy', 'vz', 'wb', 'wc', 'wd', 'wf', 'wg', 'wj', 'wk', 'wl', 'wm', 'wp', 'wq', 'wr', 'wt', 'wu', 'wv', 'ww', 'wx', 'wy', 'wz', 'xa', 'xb', 'xc', 'xd', 'xe', 'xf', 'xg', 'xh', 'xj', 'xk', 'xl', 'xm', 'xn', 'xo', 'xq', 'xr', 'xs', 'xt', 'xu', 'xv', 'xw', 'xx', 'xy', 'xz', 'ya', 'yb', 'yc', 'yd', 'ye', 'yf', 'yg', 'yh', 'yi', 'yj', 'yk', 'yl', 'ym', 'yn', 'yo', 'yp', 'yq', 'yr', 'ys', 'yu', 'yv', 'yw', 'yx', 'yy', 'yz', 'za', 'zb', 'zc', 'zd', 'ze', 'zf', 'zg', 'zh', 'zi', 'zj', 'zk', 'zl', 'zm', 'zn', 'zo', 'zp', 'zq', 'zr', 'zs', 'zt', 'zu', 'zv', 'zw', 'zx', 'zy', 'zz'] 474\n"
     ]
    }
   ],
   "source": [
    "attested_bigrams = []\n",
    "unattested_bigrams = english_bigrams\n",
    "new_text = \"\"\n",
    "n = 2\n",
    "for char in text.lower():\n",
    "    if char in alphabet or char == \" \":\n",
    "        new_text += char\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "words = new_text.split(\" \")\n",
    "for word in words:\n",
    "    for i in range(len(word) - n+1):\n",
    "        bigram = word[i:i+n]\n",
    "        if bigram not in attested_bigrams:\n",
    "            attested_bigrams.append(bigram)\n",
    "i = 0\n",
    "for bigram in unattested_bigrams:\n",
    "    if bigram in attested_bigrams:\n",
    "        del unattested_bigrams[i]\n",
    "    i += 1\n",
    "    \n",
    "print()\n",
    "print(\"Bigrams of the sample text:\", attested_bigrams, len(attested_bigrams))\n",
    "print()\n",
    "print(\"Unattested bigrams:\", unattested_bigrams, len(unattested_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4.** You are given two lists, `cities` and `small_cities`. Insert the elements from `small_cities` in `cities` in such a way so that the list `cities` would contain items in the following order:\n",
    "    \n",
    "    [\"NYC\", \"LA\", \"Stony Brook\", \"Provo\", \"SF\"]\n",
    "    \n",
    "Use any method or way you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NYC', 'LA', 'Stony Brook', 'Provo', 'SF']\n"
     ]
    }
   ],
   "source": [
    "cities = [\"NYC\", \"LA\", \"SF\"]\n",
    "small_cities = [\"Stony Brook\", \"Provo\"]\n",
    "\n",
    "# your code here\n",
    "sf = cities.pop()\n",
    "for city in small_cities:\n",
    "    cities.append(city)\n",
    "cities.append(sf)\n",
    "print(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.** Using the given list `cities`, produce the following output:\n",
    "\n",
    "    NYC NYC\n",
    "    NYC LA\n",
    "    NYC SF\n",
    "    LA NYC\n",
    "    LA LA\n",
    "    LA SF\n",
    "    SF NYC\n",
    "    SF LA\n",
    "    SF SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC NYC\n",
      "NYC LA\n",
      "NYC SF\n",
      "LA NYC\n",
      "LA LA\n",
      "LA SF\n",
      "SF NYC\n",
      "SF LA\n",
      "SF SF\n"
     ]
    }
   ],
   "source": [
    "cities = [\"NYC\", \"LA\", \"SF\"]\n",
    "\n",
    "# your code here\n",
    "for city1 in cities:\n",
    "\tfor city2 in cities:\n",
    "\t\tprint(city1, city2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6.** You are given some words from the [Swadesh list](https://en.wikipedia.org/wiki/Swadesh_list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"sun\", \"moon\", \"earth\", \"water\", \"food\", \"sky\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you are working with a native speaker of some language other than English. Create a new (empty) list for words of that language, call it `translations`. Then, for every word of the Swadesh list (`words`), ask the user to provide its translation, and save them into the `translations` list. After all the words were translated, print `translations`. (ps. for the sake of the hw, you do not need to speak another language! To test your code you can simply input the English word again! What matters is the mapping!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a translation for sun: sol\n",
      "Enter a translation for moon: luna\n",
      "Enter a translation for earth: tierra\n",
      "Enter a translation for water: agua\n",
      "Enter a translation for food: comida\n",
      "Enter a translation for sky: cielo\n",
      "['sol', 'luna', 'tierra', 'agua', 'comida', 'cielo']\n"
     ]
    }
   ],
   "source": [
    "translations = []\n",
    "for word in words:\n",
    "    translation = input(\"Enter a translation for \" + word + \": \")\n",
    "    translations.append(translation)\n",
    "print(translations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e2c3a95197615e0f03f2cb0b412ba0410d0dc2251f9ffa66ba54e74ad00933f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
